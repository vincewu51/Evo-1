
# Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment

[![üìÑ Paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/2511.04555)  

[![ü§ó HuggingFace Models](https://img.shields.io/badge/HuggingFace-Models-yellow)](https://huggingface.co/MINT-SJTU/Evo-1/tree/main)  

[![üì¶ Dataset](https://img.shields.io/badge/HuggingFace-Dataset_MetaWorld-orange)](https://huggingface.co/datasets/MINT-SJTU/Evo1_MetaWorld/tree/main)  





## üì∞ News  

- üóìÔ∏è **2025-11-06** ‚Äî Released Meta-World & LIBERO evaluation scripts  
- üóìÔ∏è **2025-11-06** ‚Äî Uploaded model weights to HuggingFace  
- üóìÔ∏è **2025-11-06** ‚Äî Released official code  




## ‚úÖ To-Do List  

- ‚¨ú Release inference script in xarm6 
- ‚¨ú Add Evo-1 to the LeRobot framework for SO100   
- ‚¨ú Release results of all 50 RoboTwin tasks
- ‚¨ú Release RoboTwin evaluation script  
  



## ‚öôÔ∏è Installation

Prepare the environment for Evo-1

```bash
# Clone this repo
git clone https://github.com/MINT-SJTU/Evo-1.git

# Create a Conda environment
conda create -n Evo1 python=3.10 -y
conda activate Evo1

# Install requirements
cd Evo_1
pip install -r requirements.txt

# You may need to reduce the MAX_JOBS to suit your computer
MAX_JOBS=64 pip install -v flash-attn --no-build-isolation
```

##  Simulation Benchmark

### üß™ Meta-World Benchmark

### 1Ô∏è‚É£ Prepare the environment for Meta-World

```bash
conda create -n metaworld python=3.10 -y
conda activate metaworld
pip install mujoco
pip install metaworld
pip install websockets
pip install opencv-python
pip install packaging
```

### 2Ô∏è‚É£ Model Preparation

### üì• 2.1 Download Model Weight

[Meta-World Evaluation Checkpoint](https://huggingface.co/MINT-SJTU/Evo-1/tree/main/Evo1_Simulation_Benchmark_Checkpoints/MetaWorld/Evo1_MetaWorld_checkpoint)


### ‚úèÔ∏è 2.2 Modify config

Modify checkpoint dir: [Evo1_server.py#L149](Evo_1/scripts/Evo1_server.py#L149)  
(Optional) Modify server port: [Evo1_server.py#L152](Evo_1/scripts/Evo1_server.py#L152)  
(Optional) Modify client port: [mt50_evo1_client_prompt.py#L40](MetaWorld_evaluation/mt50_evo1_client_prompt.py#L40)


### 3Ô∏è‚É£ Run Meta-World Evaluation

```bash
# Terminal 1
conda activate Evo1

cd Evo_1

python scripts/Evo1_server.py
```

```bash
# Terminal 2
conda activate metaworld

cd MetaWorld_evaluation

python mt50_evo1_client_prompt.py
```

---

### üß™ LIBERO Benchmark

### 1Ô∏è‚É£ Prepare the environment for LIBERO

```bash
conda create -n libero python=3.8.13 -y

conda activate libero

cd LIBERO_evaluation/

git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git

cd LIBERO

pip install -r requirements.txt

pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

pip install -e .

pip install websockets
```

### 2Ô∏è‚É£ Model Preparation

### üì• 2.1 Download Model Weight

[LIBERO Evaluation Checkpoint](https://huggingface.co/MINT-SJTU/Evo-1/tree/main/Evo1_Simulation_Benchmark_Checkpoints/LIBERO/Evo1_LIBERO_checkpoint)



### ‚úèÔ∏è 2.2 Modify config
Modify checkpoint dir: [Evo1_server.py#L149](Evo_1/scripts/Evo1_server.py#L149)  
Modify ckpt name: [libero_client_4tasks.py#L24](LIBERO_evaluation/libero_client_4tasks.py#L24)  
(Optional) Modify server port: [Evo1_server.py#L152](Evo_1/scripts/Evo1_server.py#L152)  
(Optional) Modify client port: [libero_client_4tasks.py#L23](LIBERO_evaluation/libero_client_4tasks.py#L23)  


#### 3Ô∏è‚É£ Run LIBERO Evaluation

```bash
# Terminal 1
conda activate Evo1

cd Evo_1

python scripts/Evo1_server.py
```

```bash
# Terminal 2
conda activate libero

cd LIBERO_evaluation

python libero_client_4tasks.py
```

## üß† Training on Your Own Dataset

We support **lerobot v2.1** format, please convert your data to this format.

We use MetaWorld Dataset here as an example.

### üì• 2.1 Download Dataset

```bash
mkdir Evo1_training_dataset/

cd Evo1_training_dataset/

GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/MINT-SJTU/Evo1_MetaWorld

cd Evo1_MetaWorld/

git lfs pull
```

### ‚úèÔ∏è 2.2 Modify config

### ‚úèÔ∏è 2.2.1 Modify config.yaml

You need to modify the [config.yaml](Evo_1/dataset/config.yaml)

This is used to set the dataset path and the camera mapping.

### ‚úèÔ∏è 2.2 Set the cache path

You need to change the [cache_dir](Evo_1/dataset/lerobot_dataset_pretrain_mp.py#L174)

Set the cache path so the dataset can be loaded from .pkl files next time for faster loading.

### üöÄ 3 Start Training

We use the two-stage training paradigm.

### üöÄ 3.1 Setup deepspeed
```bash
accelerate config     
```
You can check this [setup guide](deepspeed_steup_example.txt)


### üöÄ 3.2 Stage 1

We only train the integration module and action expert in stage 1.   

If you are training with multiple GPU, set --num_processes to the GPU number.  
You need to change the --run_name,--save_dir,--resume_path base on your own config.

```bash
conda activate Evo1

cd Evo_1/

accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Evo1_metaworld_stage1 --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 5000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/checkpoints/stage1
```

### üöÄ 3.3 Stage 2
We perform Full-scale training in stage 2.   

```bash
conda activate Evo1

cd Evo_1/

accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Evo1_metaworld_stage2 --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 80000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_vlm --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/checkpoints/stage2 --resume --resume_pretrain --resume_path /your/path/checkpoints/stage1/step_5000
```

### üöÄ 3.4 (Optional) Resume
If you want to resume the training process, you can use the following command (we use stage 2 as an example):

```bash
accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Your_own_name --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 80000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_vlm --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/to/save/the/checkpoints/ --resume  --resume_path /the/checkpoint/path/you/want/to/resume/from/step_20000
```

## üìö Citation
```bash
@misc{lin2025evo1lightweightvisionlanguageactionmodel,
      title={Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment}, 
      author={Tao Lin and Yilei Zhong and Yuxin Du and Jingjing Zhang and Jiting Liu and Yinxinyu Chen and Encheng Gu and Ziyan Liu and Hongyi Cai and Yanwen Zou and Lixing Zou and Zhaoye Zhou and Gen Li and Bo Zhao},
      year={2025},
      eprint={2511.04555},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2511.04555}, 
}
```

## üì¨ Contact

If you encounter any issues or have suggestions,  
please open an issue or start a discussion on GitHub.  
We sincerely welcome your feedback and contributions.

You can also scan the QR code below to connect with me or join chatting group on WeChat:


<p align="center">
<img src="readme_pics/taolin.jpg" width="200" height="300">
<img src="readme_pics/wechat_group.jpg" width="200" height="300">
  
</p>
